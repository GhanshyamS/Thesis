{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepCovid_Dataset_and_Implementation_in_PyTorch_SqueezeNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWtd89blcgj3"
      },
      "source": [
        "Deep-COVID: Predicting COVID-19 From Chest X-Ray Images Using Deep Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh9jjzj5EbV_"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import copy, pickle, os, time\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgaDkXgnHNXA",
        "outputId": "3cd252ec-bde1-40dc-ff1b-9c9faaaa6f88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDhtO5nQElzQ"
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"./data/hymenoptera_data\"\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"squeezenet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 100\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "data_dir ='drive/MyDrive/Thesis/Baseline/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgJ1GRWlH-Bg"
      },
      "source": [
        "start_time= time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ENc9_IIBZh"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'test':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6fD4ivgIEiH"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ENXTRn_mUQg",
        "outputId": "5b520a5b-38cc-4660-b416-9fbaa8f14ce8"
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TCSiOH4mX0J",
        "outputId": "10f770e0-7eae-4af7-9f1b-1bb86f0557ce"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQHJDLmrmz-O",
        "outputId": "16f67f51-312a-451e-dcfd-83247aeaac4d"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t classifier.1.weight\n",
            "\t classifier.1.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEwAP8QFnAax",
        "outputId": "bdcb5156-4df4-48ed-da86-3dd6834cf566"
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.7794 Acc: 0.5583\n",
            "test Loss: 0.6956 Acc: 0.5500\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 0.6029 Acc: 0.6667\n",
            "test Loss: 0.6423 Acc: 0.5500\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 0.5704 Acc: 0.7000\n",
            "test Loss: 0.5846 Acc: 0.6750\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.3868 Acc: 0.8083\n",
            "test Loss: 0.5331 Acc: 0.6750\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.4709 Acc: 0.7917\n",
            "test Loss: 0.6745 Acc: 0.5750\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.3522 Acc: 0.7917\n",
            "test Loss: 0.6047 Acc: 0.6250\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.3047 Acc: 0.8333\n",
            "test Loss: 0.4941 Acc: 0.6500\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.2413 Acc: 0.9083\n",
            "test Loss: 0.5868 Acc: 0.6250\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.2879 Acc: 0.8917\n",
            "test Loss: 0.6641 Acc: 0.6500\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.2399 Acc: 0.9083\n",
            "test Loss: 0.2515 Acc: 0.8500\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.2182 Acc: 0.9167\n",
            "test Loss: 0.2017 Acc: 0.9250\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.2198 Acc: 0.9250\n",
            "test Loss: 0.4193 Acc: 0.8000\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.2163 Acc: 0.8917\n",
            "test Loss: 0.1612 Acc: 0.9500\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.3033 Acc: 0.9000\n",
            "test Loss: 0.4601 Acc: 0.7500\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.2186 Acc: 0.9500\n",
            "test Loss: 0.2232 Acc: 0.9000\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.2237 Acc: 0.9083\n",
            "test Loss: 0.2549 Acc: 0.8750\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.2897 Acc: 0.8917\n",
            "test Loss: 0.1728 Acc: 0.9500\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.1548 Acc: 0.9417\n",
            "test Loss: 0.2606 Acc: 0.8500\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.2298 Acc: 0.9250\n",
            "test Loss: 0.1148 Acc: 1.0000\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.1353 Acc: 0.9667\n",
            "test Loss: 0.3901 Acc: 0.7500\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.1905 Acc: 0.9000\n",
            "test Loss: 0.2162 Acc: 0.9000\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.2204 Acc: 0.9083\n",
            "test Loss: 0.4481 Acc: 0.7000\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.1503 Acc: 0.9417\n",
            "test Loss: 0.0836 Acc: 1.0000\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.1643 Acc: 0.9250\n",
            "test Loss: 0.6535 Acc: 0.6750\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.2051 Acc: 0.9083\n",
            "test Loss: 0.6873 Acc: 0.6750\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.1556 Acc: 0.9333\n",
            "test Loss: 0.2272 Acc: 0.8250\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.1044 Acc: 0.9750\n",
            "test Loss: 0.1733 Acc: 0.9250\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.2346 Acc: 0.9083\n",
            "test Loss: 0.1815 Acc: 0.9250\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.1563 Acc: 0.9333\n",
            "test Loss: 0.3088 Acc: 0.8500\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.2149 Acc: 0.9250\n",
            "test Loss: 0.7645 Acc: 0.6750\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.2158 Acc: 0.8833\n",
            "test Loss: 1.5464 Acc: 0.5750\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.2888 Acc: 0.8917\n",
            "test Loss: 0.1280 Acc: 0.9250\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.2049 Acc: 0.9250\n",
            "test Loss: 0.1244 Acc: 0.9500\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.1882 Acc: 0.9250\n",
            "test Loss: 0.8933 Acc: 0.6750\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.2268 Acc: 0.8917\n",
            "test Loss: 0.1270 Acc: 1.0000\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.1776 Acc: 0.9167\n",
            "test Loss: 0.2988 Acc: 0.8250\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.1294 Acc: 0.9500\n",
            "test Loss: 0.4149 Acc: 0.7750\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.1402 Acc: 0.9417\n",
            "test Loss: 0.3335 Acc: 0.8000\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.1860 Acc: 0.9000\n",
            "test Loss: 0.2017 Acc: 0.8750\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.1907 Acc: 0.9250\n",
            "test Loss: 0.3112 Acc: 0.8250\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.1273 Acc: 0.9500\n",
            "test Loss: 0.3181 Acc: 0.8000\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.1151 Acc: 0.9500\n",
            "test Loss: 0.4895 Acc: 0.7250\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.1476 Acc: 0.9250\n",
            "test Loss: 0.1681 Acc: 0.8750\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.1136 Acc: 0.9500\n",
            "test Loss: 0.3100 Acc: 0.8500\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.0985 Acc: 0.9667\n",
            "test Loss: 0.4050 Acc: 0.7750\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.1151 Acc: 0.9500\n",
            "test Loss: 0.2279 Acc: 0.8500\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.1732 Acc: 0.9083\n",
            "test Loss: 0.7744 Acc: 0.6750\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.1359 Acc: 0.9500\n",
            "test Loss: 0.3595 Acc: 0.7750\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.1550 Acc: 0.9500\n",
            "test Loss: 0.3210 Acc: 0.8250\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.1283 Acc: 0.9333\n",
            "test Loss: 0.2963 Acc: 0.8750\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.1284 Acc: 0.9667\n",
            "test Loss: 0.3693 Acc: 0.8250\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.1705 Acc: 0.9250\n",
            "test Loss: 0.3296 Acc: 0.8500\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.0994 Acc: 0.9583\n",
            "test Loss: 0.4440 Acc: 0.8000\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.0980 Acc: 0.9583\n",
            "test Loss: 0.2374 Acc: 0.8500\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.1142 Acc: 0.9583\n",
            "test Loss: 0.4203 Acc: 0.8250\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.1046 Acc: 0.9750\n",
            "test Loss: 0.0966 Acc: 0.9750\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.1379 Acc: 0.9333\n",
            "test Loss: 0.0879 Acc: 1.0000\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.1644 Acc: 0.9250\n",
            "test Loss: 0.2239 Acc: 0.8500\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.1871 Acc: 0.8917\n",
            "test Loss: 1.1836 Acc: 0.6500\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.2212 Acc: 0.9083\n",
            "test Loss: 0.4336 Acc: 0.8000\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.1411 Acc: 0.9333\n",
            "test Loss: 0.4970 Acc: 0.8000\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.1460 Acc: 0.9500\n",
            "test Loss: 0.4063 Acc: 0.8000\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.0867 Acc: 0.9750\n",
            "test Loss: 0.2800 Acc: 0.8500\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.1510 Acc: 0.9500\n",
            "test Loss: 0.2188 Acc: 0.8750\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.1468 Acc: 0.9500\n",
            "test Loss: 0.6752 Acc: 0.7500\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.0965 Acc: 0.9500\n",
            "test Loss: 0.3022 Acc: 0.8500\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.1060 Acc: 0.9667\n",
            "test Loss: 0.2125 Acc: 0.8750\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.1171 Acc: 0.9750\n",
            "test Loss: 0.5800 Acc: 0.7750\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.1824 Acc: 0.9417\n",
            "test Loss: 0.1816 Acc: 0.8750\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.1522 Acc: 0.9250\n",
            "test Loss: 0.1173 Acc: 0.9500\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.1886 Acc: 0.9000\n",
            "test Loss: 0.2328 Acc: 0.8750\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.1332 Acc: 0.9417\n",
            "test Loss: 0.3562 Acc: 0.8000\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.1311 Acc: 0.9333\n",
            "test Loss: 0.2330 Acc: 0.8750\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.1556 Acc: 0.9417\n",
            "test Loss: 0.1248 Acc: 0.9750\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.0950 Acc: 0.9833\n",
            "test Loss: 0.2496 Acc: 0.8500\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.1275 Acc: 0.9500\n",
            "test Loss: 0.5321 Acc: 0.8000\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.1241 Acc: 0.9583\n",
            "test Loss: 0.3506 Acc: 0.8500\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.1139 Acc: 0.9500\n",
            "test Loss: 0.1980 Acc: 0.8750\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.1251 Acc: 0.9500\n",
            "test Loss: 0.3973 Acc: 0.8250\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.0794 Acc: 0.9667\n",
            "test Loss: 0.2656 Acc: 0.8500\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.1247 Acc: 0.9667\n",
            "test Loss: 0.4310 Acc: 0.8000\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.1235 Acc: 0.9583\n",
            "test Loss: 0.4522 Acc: 0.8000\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.1459 Acc: 0.9250\n",
            "test Loss: 0.5032 Acc: 0.7750\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.0611 Acc: 0.9833\n",
            "test Loss: 0.1882 Acc: 0.8500\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.0654 Acc: 0.9833\n",
            "test Loss: 0.2929 Acc: 0.8250\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.1065 Acc: 0.9583\n",
            "test Loss: 0.1497 Acc: 0.9000\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.1315 Acc: 0.9417\n",
            "test Loss: 0.4932 Acc: 0.7750\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.1137 Acc: 0.9583\n",
            "test Loss: 0.1497 Acc: 0.8750\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.0659 Acc: 0.9833\n",
            "test Loss: 0.3277 Acc: 0.8250\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.1213 Acc: 0.9583\n",
            "test Loss: 0.1063 Acc: 0.9500\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.2110 Acc: 0.9250\n",
            "test Loss: 0.1417 Acc: 0.8750\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.1512 Acc: 0.9500\n",
            "test Loss: 0.2880 Acc: 0.8500\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.0648 Acc: 0.9917\n",
            "test Loss: 0.3141 Acc: 0.8500\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.0815 Acc: 0.9750\n",
            "test Loss: 0.2633 Acc: 0.8500\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.1209 Acc: 0.9500\n",
            "test Loss: 0.3305 Acc: 0.8500\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.0921 Acc: 0.9583\n",
            "test Loss: 0.2528 Acc: 0.8500\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.0685 Acc: 0.9833\n",
            "test Loss: 0.1305 Acc: 0.9500\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.0585 Acc: 0.9833\n",
            "test Loss: 0.2681 Acc: 0.8500\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.0551 Acc: 1.0000\n",
            "test Loss: 0.3330 Acc: 0.8500\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.0567 Acc: 0.9750\n",
            "test Loss: 0.2830 Acc: 0.8500\n",
            "\n",
            "Training complete in 3m 2s\n",
            "Best val Acc: 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWXkDEQSvfQQ"
      },
      "source": [
        "def getConfusionMatrix(model, show_image=False):\n",
        "    model.eval() #set the model to evaluation mode\n",
        "    confusion_matrix=np.zeros((2,2),dtype=int) #initialize a confusion matrix\n",
        "    num_images=dataloaders_dict['test'] #size of the testset\n",
        "    \n",
        "    with torch.no_grad(): #disable back prop to test the model\n",
        "        for i, (inputs, labels) in enumerate(dataloaders_dict['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #get predictions of the model\n",
        "            outputs = model(inputs) \n",
        "            _, preds = torch.max(outputs, 1) \n",
        "            \n",
        "            #get confusion matrix\n",
        "            for j in range(inputs.size()[0]): \n",
        "                if preds[j]==1 and labels[j]==1:\n",
        "                    term='TP'\n",
        "                    confusion_matrix[0][0]+=1\n",
        "                elif preds[j]==1 and labels[j]==0:\n",
        "                    term='FP'\n",
        "                    confusion_matrix[1][0]+=1\n",
        "                elif preds[j]==0 and labels[j]==1:\n",
        "                    term='FN'\n",
        "                    confusion_matrix[0][1]+=1\n",
        "                elif preds[j]==0 and labels[j]==0:\n",
        "                    term='TN'\n",
        "                    confusion_matrix[1][1]+=1\n",
        "                #show image and its class in confusion matrix    \n",
        "                if show_image:\n",
        "                    print('predicted: {}'.format(class_names[preds[j]]))\n",
        "                    print(term)\n",
        "                    imshow(inputs.cpu().data[j])\n",
        "                    print()\n",
        "        #print results\n",
        "        print('Confusion Matrix: ')\n",
        "        print(confusion_matrix)\n",
        "        Sensitivity = confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])\n",
        "        Specificity = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])\n",
        "        PPV = confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])\n",
        "        NPV = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
        "        print()\n",
        "        print('Sensitivity: ', 100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1]))\n",
        "        print('Specificity: ', 100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0]))\n",
        "        print('PPV: ', 100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0]))\n",
        "        print('NPV: ', 100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1]))\n",
        "        print('F1Score: ',100*(2*(PPV*Sensitivity))/(PPV + Sensitivity))\n",
        "        return confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQINa2UPvhMo",
        "outputId": "cb41b728-a13d-4fed-eb3f-db8c0620abd8"
      },
      "source": [
        "getConfusionMatrix(model_ft)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix: \n",
            "[[20  0]\n",
            " [ 0 20]]\n",
            "\n",
            "Sensitivity:  100.0\n",
            "Specificity:  100.0\n",
            "PPV:  100.0\n",
            "NPV:  100.0\n",
            "F1Score:  100.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[20,  0],\n",
              "       [ 0, 20]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}