{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepCovid_Dataset_and_Implementation_in_PyTorch_AlexNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f663d91173449f9aeceae82d8a9cf55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e00092d35d564a6d9bdb925932950b61",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_389fa6e8716b45d681f6945bccda6426",
              "IPY_MODEL_a887617f1d4c424992bef74a5c512106"
            ]
          }
        },
        "e00092d35d564a6d9bdb925932950b61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "389fa6e8716b45d681f6945bccda6426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7cb0debe7ae9446fb96462a358113631",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244408911,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244408911,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c4d18deef864aefa9fa292e2fb61b38"
          }
        },
        "a887617f1d4c424992bef74a5c512106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3f75b75d7efe455f9502d8942596d422",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [01:09&lt;00:00, 3.51MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07be3ac75f3d4ee6985dd05dda4ad34a"
          }
        },
        "7cb0debe7ae9446fb96462a358113631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c4d18deef864aefa9fa292e2fb61b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f75b75d7efe455f9502d8942596d422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07be3ac75f3d4ee6985dd05dda4ad34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejkcHp41K7Ik"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import copy, pickle, os, time\n",
        "import argparse"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8S4U0j3LFea",
        "outputId": "77c5ee6f-fb69-479c-81a3-db1e3e7001e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtY_vZlKLHkk"
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"alexnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 100\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "data_dir ='drive/MyDrive/Thesis/Baseline/data/'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH1wEm3gLeqi"
      },
      "source": [
        "start_time= time.time()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "255PBlGSLgui"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'test':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39eekOeqLknc"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "0f663d91173449f9aeceae82d8a9cf55",
            "e00092d35d564a6d9bdb925932950b61",
            "389fa6e8716b45d681f6945bccda6426",
            "a887617f1d4c424992bef74a5c512106",
            "7cb0debe7ae9446fb96462a358113631",
            "4c4d18deef864aefa9fa292e2fb61b38",
            "3f75b75d7efe455f9502d8942596d422",
            "07be3ac75f3d4ee6985dd05dda4ad34a"
          ]
        },
        "id": "8aYV9qH6Lm3S",
        "outputId": "82b596cf-eeaf-4a49-fd70-192454cd7e4a"
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f663d91173449f9aeceae82d8a9cf55",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244408911.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJeJq6PnL2M2",
        "outputId": "886de1a3-e4e0-4f19-8b81-8ee1297e06dc"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXhdJp9cL3UT",
        "outputId": "fb496650-6bbb-41a2-b4e1-9826005a14c8"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhHgOyO4L8b7",
        "outputId": "de6da98d-04e0-4e59-ad76-8c7e95ac25c7"
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.6652 Acc: 0.6833\n",
            "test Loss: 0.7013 Acc: 0.6500\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 0.4959 Acc: 0.7667\n",
            "test Loss: 0.5617 Acc: 0.6750\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 0.5463 Acc: 0.7833\n",
            "test Loss: 0.1132 Acc: 0.9750\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.4817 Acc: 0.7833\n",
            "test Loss: 0.5364 Acc: 0.7500\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.5146 Acc: 0.8333\n",
            "test Loss: 0.6108 Acc: 0.7750\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.2674 Acc: 0.9083\n",
            "test Loss: 0.7865 Acc: 0.7500\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.3709 Acc: 0.8333\n",
            "test Loss: 0.7179 Acc: 0.7500\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.2879 Acc: 0.8917\n",
            "test Loss: 0.3731 Acc: 0.8250\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.4030 Acc: 0.8250\n",
            "test Loss: 0.0558 Acc: 1.0000\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.3037 Acc: 0.9083\n",
            "test Loss: 0.1200 Acc: 0.9500\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.3665 Acc: 0.8750\n",
            "test Loss: 0.5880 Acc: 0.8250\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.2918 Acc: 0.9167\n",
            "test Loss: 0.1265 Acc: 0.9750\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.2750 Acc: 0.9167\n",
            "test Loss: 0.3730 Acc: 0.8750\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.2522 Acc: 0.9083\n",
            "test Loss: 0.2454 Acc: 0.8750\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.2792 Acc: 0.9167\n",
            "test Loss: 0.8044 Acc: 0.6750\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.1584 Acc: 0.9250\n",
            "test Loss: 0.1178 Acc: 0.9500\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.2279 Acc: 0.9000\n",
            "test Loss: 1.1375 Acc: 0.6500\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.3381 Acc: 0.8667\n",
            "test Loss: 0.1546 Acc: 0.9750\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.2982 Acc: 0.8833\n",
            "test Loss: 0.1678 Acc: 0.9500\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.2709 Acc: 0.8750\n",
            "test Loss: 0.0762 Acc: 0.9750\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.3281 Acc: 0.8750\n",
            "test Loss: 1.2303 Acc: 0.6000\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.3112 Acc: 0.8500\n",
            "test Loss: 0.4493 Acc: 0.8000\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.3686 Acc: 0.8833\n",
            "test Loss: 0.0636 Acc: 0.9750\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.2394 Acc: 0.9167\n",
            "test Loss: 0.4861 Acc: 0.8500\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.3512 Acc: 0.8833\n",
            "test Loss: 0.7171 Acc: 0.7250\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.3343 Acc: 0.9000\n",
            "test Loss: 0.2582 Acc: 0.9500\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.3446 Acc: 0.8500\n",
            "test Loss: 0.2146 Acc: 0.9500\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.3083 Acc: 0.8833\n",
            "test Loss: 0.1253 Acc: 0.9750\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.2935 Acc: 0.8917\n",
            "test Loss: 0.1045 Acc: 0.9750\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.3427 Acc: 0.8667\n",
            "test Loss: 0.1491 Acc: 0.9750\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.3438 Acc: 0.8667\n",
            "test Loss: 0.6745 Acc: 0.7500\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.2711 Acc: 0.9250\n",
            "test Loss: 0.2415 Acc: 0.9250\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.1388 Acc: 0.9583\n",
            "test Loss: 0.5507 Acc: 0.8250\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.2733 Acc: 0.8917\n",
            "test Loss: 0.2454 Acc: 0.8750\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.1507 Acc: 0.9250\n",
            "test Loss: 0.3518 Acc: 0.8500\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.1388 Acc: 0.9500\n",
            "test Loss: 0.1181 Acc: 0.9500\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.2288 Acc: 0.9250\n",
            "test Loss: 0.0827 Acc: 0.9750\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.3097 Acc: 0.8833\n",
            "test Loss: 0.4426 Acc: 0.8000\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.2459 Acc: 0.9000\n",
            "test Loss: 0.3744 Acc: 0.8500\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.3222 Acc: 0.8917\n",
            "test Loss: 0.2540 Acc: 0.9250\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.3027 Acc: 0.9000\n",
            "test Loss: 0.2374 Acc: 0.9250\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.3669 Acc: 0.9000\n",
            "test Loss: 0.0472 Acc: 0.9750\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.2254 Acc: 0.9000\n",
            "test Loss: 0.0572 Acc: 0.9750\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.3598 Acc: 0.9250\n",
            "test Loss: 0.5811 Acc: 0.7500\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.2775 Acc: 0.8750\n",
            "test Loss: 0.2840 Acc: 0.9000\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.6439 Acc: 0.8250\n",
            "test Loss: 0.0394 Acc: 0.9750\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.4013 Acc: 0.8750\n",
            "test Loss: 0.0228 Acc: 1.0000\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.5081 Acc: 0.8333\n",
            "test Loss: 0.4616 Acc: 0.8750\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.2434 Acc: 0.9000\n",
            "test Loss: 0.9398 Acc: 0.7500\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.7198 Acc: 0.8250\n",
            "test Loss: 0.0407 Acc: 0.9750\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.3742 Acc: 0.9167\n",
            "test Loss: 0.1839 Acc: 0.9500\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.2082 Acc: 0.9167\n",
            "test Loss: 0.0922 Acc: 0.9750\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.3123 Acc: 0.8917\n",
            "test Loss: 0.3819 Acc: 0.9250\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.1933 Acc: 0.9250\n",
            "test Loss: 0.4066 Acc: 0.9000\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.1899 Acc: 0.9417\n",
            "test Loss: 0.3429 Acc: 0.9000\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.2655 Acc: 0.9000\n",
            "test Loss: 0.3524 Acc: 0.9000\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.2444 Acc: 0.9417\n",
            "test Loss: 0.1724 Acc: 0.9750\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.3166 Acc: 0.9000\n",
            "test Loss: 0.4187 Acc: 0.8500\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.2379 Acc: 0.9167\n",
            "test Loss: 0.3123 Acc: 0.9000\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.1784 Acc: 0.9333\n",
            "test Loss: 0.2011 Acc: 0.9750\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.2045 Acc: 0.9333\n",
            "test Loss: 0.3405 Acc: 0.9000\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.1723 Acc: 0.9417\n",
            "test Loss: 0.5105 Acc: 0.8500\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.1441 Acc: 0.9333\n",
            "test Loss: 0.2649 Acc: 0.9500\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.4263 Acc: 0.8833\n",
            "test Loss: 0.8064 Acc: 0.8000\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.2639 Acc: 0.9000\n",
            "test Loss: 0.4938 Acc: 0.8500\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.2202 Acc: 0.9083\n",
            "test Loss: 0.6667 Acc: 0.8250\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.3478 Acc: 0.8667\n",
            "test Loss: 0.2603 Acc: 0.9000\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.1913 Acc: 0.9167\n",
            "test Loss: 0.0784 Acc: 0.9500\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.2219 Acc: 0.9083\n",
            "test Loss: 0.3016 Acc: 0.8750\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.3410 Acc: 0.8750\n",
            "test Loss: 0.1837 Acc: 0.9250\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.2740 Acc: 0.9083\n",
            "test Loss: 0.3773 Acc: 0.8750\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.3038 Acc: 0.9000\n",
            "test Loss: 0.2545 Acc: 0.9250\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.1807 Acc: 0.9417\n",
            "test Loss: 0.1897 Acc: 0.9500\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.1546 Acc: 0.9250\n",
            "test Loss: 0.8889 Acc: 0.7250\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.4527 Acc: 0.8750\n",
            "test Loss: 0.1586 Acc: 0.9750\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.1529 Acc: 0.9167\n",
            "test Loss: 0.2011 Acc: 0.9750\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.2280 Acc: 0.9250\n",
            "test Loss: 0.3697 Acc: 0.8750\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.2026 Acc: 0.9250\n",
            "test Loss: 0.1305 Acc: 0.9500\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.2453 Acc: 0.9000\n",
            "test Loss: 0.6595 Acc: 0.8000\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.4599 Acc: 0.8667\n",
            "test Loss: 0.4410 Acc: 0.8500\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.1812 Acc: 0.9417\n",
            "test Loss: 0.2207 Acc: 0.9250\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.2080 Acc: 0.9417\n",
            "test Loss: 0.2886 Acc: 0.9000\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.1590 Acc: 0.9333\n",
            "test Loss: 0.3848 Acc: 0.8750\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.2160 Acc: 0.9250\n",
            "test Loss: 0.2834 Acc: 0.9250\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.2748 Acc: 0.9083\n",
            "test Loss: 0.1768 Acc: 0.9250\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.2103 Acc: 0.9250\n",
            "test Loss: 2.0789 Acc: 0.5500\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.3650 Acc: 0.8750\n",
            "test Loss: 0.6389 Acc: 0.8750\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.1820 Acc: 0.9417\n",
            "test Loss: 0.1175 Acc: 0.9500\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.1567 Acc: 0.9417\n",
            "test Loss: 0.7860 Acc: 0.6750\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.2769 Acc: 0.9000\n",
            "test Loss: 0.2234 Acc: 0.9250\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.2941 Acc: 0.9333\n",
            "test Loss: 0.4441 Acc: 0.8250\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.3240 Acc: 0.9167\n",
            "test Loss: 0.6846 Acc: 0.7750\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.1537 Acc: 0.9250\n",
            "test Loss: 0.7619 Acc: 0.7750\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.2676 Acc: 0.8750\n",
            "test Loss: 0.0591 Acc: 0.9750\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.1313 Acc: 0.9417\n",
            "test Loss: 0.3273 Acc: 0.9000\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.2314 Acc: 0.9083\n",
            "test Loss: 0.6826 Acc: 0.7250\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.2100 Acc: 0.9250\n",
            "test Loss: 0.0584 Acc: 0.9750\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.1914 Acc: 0.9333\n",
            "test Loss: 0.5939 Acc: 0.7250\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.1436 Acc: 0.9583\n",
            "test Loss: 0.0633 Acc: 0.9750\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.1225 Acc: 0.9500\n",
            "test Loss: 0.6976 Acc: 0.7250\n",
            "\n",
            "Training complete in 11m 32s\n",
            "Best val Acc: 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfJNcuJ4L_8o"
      },
      "source": [
        "def getConfusionMatrix(model, show_image=False):\n",
        "    model.eval() #set the model to evaluation mode\n",
        "    confusion_matrix=np.zeros((2,2),dtype=int) #initialize a confusion matrix\n",
        "    num_images=dataloaders_dict['test'] #size of the testset\n",
        "    \n",
        "    with torch.no_grad(): #disable back prop to test the model\n",
        "        for i, (inputs, labels) in enumerate(dataloaders_dict['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #get predictions of the model\n",
        "            outputs = model(inputs) \n",
        "            _, preds = torch.max(outputs, 1) \n",
        "            \n",
        "            #get confusion matrix\n",
        "            for j in range(inputs.size()[0]): \n",
        "                if preds[j]==1 and labels[j]==1:\n",
        "                    term='TP'\n",
        "                    confusion_matrix[0][0]+=1\n",
        "                elif preds[j]==1 and labels[j]==0:\n",
        "                    term='FP'\n",
        "                    confusion_matrix[1][0]+=1\n",
        "                elif preds[j]==0 and labels[j]==1:\n",
        "                    term='FN'\n",
        "                    confusion_matrix[0][1]+=1\n",
        "                elif preds[j]==0 and labels[j]==0:\n",
        "                    term='TN'\n",
        "                    confusion_matrix[1][1]+=1\n",
        "                #show image and its class in confusion matrix    \n",
        "                if show_image:\n",
        "                    print('predicted: {}'.format(class_names[preds[j]]))\n",
        "                    print(term)\n",
        "                    imshow(inputs.cpu().data[j])\n",
        "                    print()\n",
        "        #print results\n",
        "        print('Confusion Matrix: ')\n",
        "        print(confusion_matrix)\n",
        "        Sensitivity = confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])\n",
        "        Specificity = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])\n",
        "        PPV = confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])\n",
        "        NPV = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
        "        print()\n",
        "        print('Sensitivity: ', 100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1]))\n",
        "        print('Specificity: ', 100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0]))\n",
        "        print('PPV: ', 100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0]))\n",
        "        print('NPV: ', 100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1]))\n",
        "        print('F1Score: ',100*(2*(PPV*Sensitivity))/(PPV + Sensitivity))\n",
        "        return confusion_matrix"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSVbzOvMMCTs",
        "outputId": "79f26bbb-4621-4c18-c2e7-4bea6d9ca606"
      },
      "source": [
        "getConfusionMatrix(model_ft)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix: \n",
            "[[20  0]\n",
            " [ 0 20]]\n",
            "\n",
            "Sensitivity:  100.0\n",
            "Specificity:  100.0\n",
            "PPV:  100.0\n",
            "NPV:  100.0\n",
            "F1Score:  100.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[20,  0],\n",
              "       [ 0, 20]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}